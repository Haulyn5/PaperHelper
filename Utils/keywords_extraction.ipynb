{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "# 设置查询参数\n",
    "params = {\n",
    "    'search': 'code',\n",
    "    'threshold': 0.0,  # 设置相似度阈值\n",
    "    'debug': 'True'   # 设置是否开启调试模式\n",
    "}\n",
    "\n",
    "# 构建查询字符串\n",
    "query_string = urllib.parse.urlencode(params)\n",
    "url = f\"http://127.0.0.1:40500/search?{query_string}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:40500/search?search=code&threshold=0.0&debug=True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 发送 GET 请求\n",
    "response = requests.get(url)\n",
    "print(url)\n",
    "# 检查响应状态并处理\n",
    "if response.status_code == 200:\n",
    "    papers = response.json()\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'abstract': 'The study explores the synergistic combination of Synthetic Aperture Radar\\n(SAR) and Visible-Near Infrared-Short Wave Infrared (VNIR-SWIR) imageries for\\nland use/land cover (LULC) classification. Image fusion, employing Bayesian\\nfusion, merges SAR texture bands with VNIR-SWIR imageries. The research aims to\\ninvestigate the impact of this fusion on LULC classification. Despite the\\npopularity of random forests for supervised classification, their limitations,\\nsuch as suboptimal performance with fewer features and accuracy stagnation, are\\naddressed. To overcome these issues, ensembles of random forests (RFE) are\\ncreated, introducing random rotations using the Forest-RC algorithm. Three\\nrotation approaches: principal component analysis (PCA), sparse random rotation\\n(SRP) matrix, and complete random rotation (CRP) matrix are employed.\\nSentinel-1 SAR data and Sentinel-2 VNIR-SWIR data from the IIT-Kanpur region\\nconstitute the training datasets, including SAR, SAR with texture, VNIR-SWIR,\\nVNIR-SWIR with texture, and fused VNIR-SWIR with texture. The study evaluates\\nclassifier efficacy, explores the impact of SAR and VNIR-SWIR fusion on\\nclassification, and significantly enhances the execution speed of Bayesian\\nfusion code. The SRP-based RFE outperforms other ensembles for the first two\\ndatasets, yielding average overall kappa values of 61.80% and 68.18%, while the\\nCRP-based RFE excels for the last three datasets with average overall kappa\\nvalues of 95.99%, 96.93%, and 96.30%. The fourth dataset achieves the highest\\noverall kappa of 96.93%. Furthermore, incorporating texture with SAR bands\\nresults in a maximum overall kappa increment of 10.00%, while adding texture to\\nVNIR-SWIR bands yields a maximum increment of approximately 3.45%.',\n",
       "  'arxiv_category': 'cs.CV, cs.AI, eess.IV',\n",
       "  'arxiv_id': '2312.10798',\n",
       "  'arxiv_upload_date': '2023-12-17T19:22:39',\n",
       "  'arxiv_url': 'https://arxiv.org/abs/2312.10798',\n",
       "  'authors': 'Shivam Pande',\n",
       "  'id': 30051,\n",
       "  'publication_date': None,\n",
       "  'publication_name': None,\n",
       "  'publication_url': None,\n",
       "  'title': 'Land use/land cover classification of fused Sentinel-1 and Sentinel-2   imageries using ensembles of Random Forests'},\n",
       " {'abstract': 'Symbolic music datasets are important for music information retrieval and\\nmusical analysis. However, there is a lack of large-scale symbolic datasets for\\nclassical piano music. In this article, we create a GiantMIDI-Piano (GP)\\ndataset containing 38,700,838 transcribed notes and 10,855 unique solo piano\\nworks composed by 2,786 composers. We extract the names of music works and the\\nnames of composers from the International Music Score Library Project (IMSLP).\\nWe search and download their corresponding audio recordings from the internet.\\nWe further create a curated subset containing 7,236 works composed by 1,787\\ncomposers by constraining the titles of downloaded audio recordings containing\\nthe surnames of composers. We apply a convolutional neural network to detect\\nsolo piano works. Then, we transcribe those solo piano recordings into Musical\\nInstrument Digital Interface (MIDI) files using a high-resolution piano\\ntranscription system. Each transcribed MIDI file contains the onset, offset,\\npitch, and velocity attributes of piano notes and pedals. GiantMIDI-Piano\\nincludes 90% live performance MIDI files and 10\\\\% sequence input MIDI files. We\\nanalyse the statistics of GiantMIDI-Piano and show pitch class, interval,\\ntrichord, and tetrachord frequencies of six composers from different eras to\\nshow that GiantMIDI-Piano can be used for musical analysis. We evaluate the\\nquality of GiantMIDI-Piano in terms of solo piano detection F1 scores, metadata\\naccuracy, and transcription error rates. We release the source code for\\nacquiring the GiantMIDI-Piano dataset at\\nhttps://github.com/bytedance/GiantMIDI-Piano',\n",
       "  'arxiv_category': 'cs.IR, cs.SD, eess.AS',\n",
       "  'arxiv_id': '2010.07061',\n",
       "  'arxiv_upload_date': '2020-10-11T01:23:43',\n",
       "  'arxiv_url': 'https://arxiv.org/abs/2010.07061',\n",
       "  'authors': 'Qiuqiang Kong, Bochen Li, Jitong Chen, Yuxuan Wang',\n",
       "  'id': 6752,\n",
       "  'publication_date': None,\n",
       "  'publication_name': None,\n",
       "  'publication_url': None,\n",
       "  'title': 'GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music'},\n",
       " {'abstract': \"Deep neural networks (DNNs) have advanced many machine learning tasks, but\\ntheir performance is often harmed by noisy labels in real-world data.\\nAddressing this, we introduce CoLafier, a novel approach that uses Local\\nIntrinsic Dimensionality (LID) for learning with noisy labels. CoLafier\\nconsists of two subnets: LID-dis and LID-gen. LID-dis is a specialized\\nclassifier. Trained with our uniquely crafted scheme, LID-dis consumes both a\\nsample's features and its label to predict the label - which allows it to\\nproduce an enhanced internal representation. We observe that LID scores\\ncomputed from this representation effectively distinguish between correct and\\nincorrect labels across various noise scenarios. In contrast to LID-dis,\\nLID-gen, functioning as a regular classifier, operates solely on the sample's\\nfeatures. During training, CoLafier utilizes two augmented views per instance\\nto feed both subnets. CoLafier considers the LID scores from the two views as\\nproduced by LID-dis to assign weights in an adapted loss function for both\\nsubnets. Concurrently, LID-gen, serving as classifier, suggests pseudo-labels.\\nLID-dis then processes these pseudo-labels along with two views to derive LID\\nscores. Finally, these LID scores along with the differences in predictions\\nfrom the two subnets guide the label update decisions. This dual-view and\\ndual-subnet approach enhances the overall reliability of the framework. Upon\\ncompletion of the training, we deploy the LID-gen subnet of CoLafier as the\\nfinal classification model. CoLafier demonstrates improved prediction accuracy,\\nsurpassing existing methods, particularly under severe label noise. For more\\ndetails, see the code at https://github.com/zdy93/CoLafier.\",\n",
       "  'arxiv_category': 'cs.LG, cs.AI',\n",
       "  'arxiv_id': '2401.05458',\n",
       "  'arxiv_upload_date': '2024-01-10T08:10:59',\n",
       "  'arxiv_url': 'https://arxiv.org/abs/2401.05458',\n",
       "  'authors': 'Dongyu Zhang, Ruofan Hu, Elke Rundensteiner',\n",
       "  'id': 35812,\n",
       "  'publication_date': None,\n",
       "  'publication_name': None,\n",
       "  'publication_url': None,\n",
       "  'title': 'CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic   Dimensionality Guidance'},\n",
       " {'abstract': 'Context: Comprehensive studies of Wolf-Rayet stars were performed in the past\\nfor the Galactic and the LMC population. The results revealed significant\\ndifferences, but also unexpected similarities between the WR populations of\\nthese different galaxies. Analyzing the WR stars in M31 will extend our\\nunderstanding of these objects in different galactic environments. Aims: The\\npresent study aims at the late-type WN stars in M31. The stellar and wind\\nparameters will tell about the formation of WR stars in other galaxies with\\ndifferent metallicity and star formation histories. The obtained parameters\\nwill provide constraints to the evolution of massive stars in the environment\\nof M31. Methods: We used the latest version of the Potsdam Wolf-Rayet model\\natmosphere code to analyze the stars via fitting optical spectra and\\nphotometric data. To account for the relatively low temperatures of the late\\nWN10 and WN11 subtypes, our WN models have been extended into this temperature\\nregime. Results: Stellar and atmospheric parameters are derived for all known\\nlate-type WN stars in M31 with available spectra. All of these stars still have\\nhydrogen in their outer envelopes, some of them up to 50% by mass. The stars\\nare located on the cool side of the zero age main sequence in the\\nHertzsprung-Russell diagram, while their luminosities range from $10^5$ to\\n$10^6$ Lsun. It is remarkable that no star exceeds $10^6$ Lsun. Conclusions: If\\nformed via single-star evolution, the late-type WN stars in M31 stem from an\\ninitial mass range between 20 and 60 Msun. From the very late-type WN9-11\\nstars, only one star is located in the S Doradus instability strip. We do not\\nfind any late-type WN stars with the high luminosities known in the Milky Way.',\n",
       "  'arxiv_category': 'astro-ph.SR, astro-ph.GA',\n",
       "  'arxiv_id': '1402.2282',\n",
       "  'arxiv_upload_date': '2014-02-10T21:00:21',\n",
       "  'arxiv_url': 'https://arxiv.org/abs/1402.2282',\n",
       "  'authors': 'Andreas Sander, Helge Todt, Rainer Hainich, Wolf-Rainer Hamann',\n",
       "  'id': 27318,\n",
       "  'publication_date': None,\n",
       "  'publication_name': None,\n",
       "  'publication_url': None,\n",
       "  'title': 'The Wolf-Rayet stars in M31: I. Analysis of the late-type WN stars'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "example = [\n",
    "    {\n",
    "        \"query\":\"deep learning for code area, like coding, program, software, etc.\",\n",
    "        \"input\":\"\"\"http://127.0.0.1:40500/search?search=code&threshold=0.1&debug=True\n",
    "[{'abstract': 'We present a novel clustering algorithm, visClust, that is based on lower\\ndimensional data representations and visual interpretation. Thereto, we design\\na transformation that allows the data to be represented by a binary integer\\narray enabling the use of image processing methods to select a partition.\\nQualitative and quantitative analyses measured in accuracy and an adjusted\\nRand-Index show that the algorithm performs well while requiring low runtime\\nand RAM. We compare the results to 6 state-of-the-art algorithms with available\\ncode, confirming the quality of visClust by superior performance in most\\nexperiments. Moreover, the algorithm asks for just one obligatory input\\nparameter while allowing optimization via optional parameters. The code is made\\navailable on GitHub and straightforward to use.',\n",
    "  'arxiv_category': 'cs.CV',\n",
    "  'arxiv_id': '2211.03894',\n",
    "  'arxiv_upload_date': '2022-11-07T22:56:23',\n",
    "  'arxiv_url': 'https://arxiv.org/abs/2211.03894',\n",
    "  'authors': 'Anna Breger, Clemens Karner, Martin Ehler',\n",
    "  'id': 25711,\n",
    "  'publication_date': None,\n",
    "  'publication_name': None,\n",
    "  'publication_url': None,\n",
    "  'title': 'visClust: A visual clustering algorithm based on orthogonal projections'},\n",
    " {'abstract': \"Software vulnerabilities are now reported at an unprecedented speed due to\\nthe recent development of automated vulnerability hunting tools. However,\\nfixing vulnerabilities still mainly depends on programmers' manual efforts.\\nDevelopers need to deeply understand the vulnerability and try to affect the\\nsystem's functions as little as possible.\\n  In this paper, with the advancement of Neural Machine Translation (NMT)\\ntechniques, we provide a novel approach called SeqTrans to exploit historical\\nvulnerability fixes to provide suggestions and automatically fix the source\\ncode. To capture the contextual information around the vulnerable code, we\\npropose to leverage data flow dependencies to construct code sequences and fed\\nthem into the state-of-the-art transformer model. The fine-tuning strategy has\\nbeen introduced to overcome the small sample size problem. We evaluate SeqTrans\\non a dataset containing 1,282 commits that fix 624 vulnerabilities in 205 Java\\nprojects. Results show that the accuracy of SeqTrans outperforms the latest\\ntechniques and achieves 23.3% in statement-level fix and 25.3% in CVE-level\\nfix. In the meantime, we look deep inside the result and observe that NMT model\\nperforms very well in certain kinds of vulnerabilities like CWE-287 (Improper\\nAuthentication) and CWE-863 (Incorrect Authorization).\",\n",
    "  'arxiv_category': 'cs.CR, cs.SE',\n",
    "  'arxiv_id': '2010.10805',\n",
    "  'arxiv_upload_date': '2020-10-21T07:49:08',\n",
    "  'arxiv_url': 'https://arxiv.org/abs/2010.10805',\n",
    "  'authors': 'Jianlei Chi, Yu Qu, Ting Liu, Qinghua Zheng, Heng Yin',\n",
    "  'id': 17090,\n",
    "  'publication_date': None,\n",
    "  'publication_name': None,\n",
    "  'publication_url': None,\n",
    "  'title': 'SeqTrans: Automatic Vulnerability Fix via Sequence to Sequence Learning'},\n",
    " {'abstract': \"While language models are powerful and versatile, they often fail to address\\nhighly complex problems. This is because solving complex problems requires\\ndeliberate thinking, which has been only minimally guided during training. In\\nthis paper, we propose a new method called Cumulative Reasoning (CR), which\\nemploys language models in a cumulative and iterative manner to emulate human\\nthought processes. By decomposing tasks into smaller components, CR streamlines\\nthe problem-solving process, rendering it both more manageable and effective.\\nFor logical inference tasks, CR consistently outperforms existing methods with\\nan improvement up to 9.3%, and achieves an accuracy of 98.04% on the curated\\nFOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy\\nof 98%, which signifies a substantial enhancement of 24% over the previous\\nstate-of-the-art method. Finally, on the MATH dataset, we establish new\\nstate-of-the-art results with 58.0% overall accuracy, surpassing the previous\\nbest approach by a margin of 4.2%, and achieving 43% relative improvement on\\nthe hardest level 5 problems (22.4% to 32.1%). Additionally, we expand the\\nconcept of Cumulative Reasoning to incorporate a Python code environment,\\ndeliberately omitting external aids such as retrieval and web browsing and\\nfocusing solely on the LLM's intrinsic reasoning capabilities within a Python\\ncode environment. Our experiments in this setting yielded impressive results,\\nwith an overall accuracy of 72.2% on the MATH dataset, significantly\\noutperforming the PAL method with 38.8% relative improvement. Code is available\\nat https://github.com/iiis-ai/cumulative-reasoning.\",\n",
    "  'arxiv_category': 'cs.AI',\n",
    "  'arxiv_id': '2308.04371',\n",
    "  'arxiv_upload_date': '2023-08-08T16:18:20',\n",
    "  'arxiv_url': 'https://arxiv.org/abs/2308.04371',\n",
    "  'authors': 'Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao',\n",
    "  'id': 25502,\n",
    "  'publication_date': None,\n",
    "  'publication_name': None,\n",
    "  'publication_url': None,\n",
    "  'title': 'Cumulative Reasoning with Large Language Models'},\n",
    " {'abstract': 'Neural Code Completion Tools (NCCTs) have reshaped the field of software\\ndevelopment, which accurately suggest contextually-relevant code snippets\\nbenefiting from language modeling techniques. However, language models may emit\\nthe training data verbatim during inference with appropriate prompts. This\\nmemorization property raises privacy concerns of commercial NCCTs about the\\nhard-coded credential leakage, leading to unauthorized access to systems.\\nTherefore, to answer whether NCCTs will inadvertently emit the hard-coded\\ncredential, we propose an evaluation tool called Hard-coded Credential Revealer\\n(HCR). HCR effectively constructs test prompts from GitHub code files with\\ncredentials to trigger memorization phenomenon of commercial NCCTs. Then, HCR\\nextracts credentials with pre-defined format from the responses by four\\ndesigned filters. We apply HCR to evaluate two representative commercial NCCTs:\\nGitHub Copilot and Amazon CodeWhisperer and successfully extracted 2,702\\nhard-coded credentials from Copilot and 129 secrets from CodeWhisper under the\\nblack-box setting, among which at least 3.6% and 5.4% secrets are real strings\\nfrom GitHub repositories. Moreover, two operational credentials were\\nidentified. The experimental results raise the severe privacy concern of the\\npotential leakage of hard-coded credentials in the training data of commercial\\nNCCTs.',\n",
    "  'arxiv_category': 'cs.CR',\n",
    "  'arxiv_id': '2309.07639',\n",
    "  'arxiv_upload_date': '2023-09-14T12:05:02',\n",
    "  'arxiv_url': 'https://arxiv.org/abs/2309.07639',\n",
    "  'authors': 'Yizhan Huang, Yichen Li, Weibin Wu, Jianping Zhang, Michael R. Lyu',\n",
    "  'id': 4587,\n",
    "  'publication_date': None,\n",
    "  'publication_name': None,\n",
    "  'publication_url': None,\n",
    "  'title': 'Do Not Give Away My Secrets: Uncovering the Privacy Issue of Neural Code   Completion Tools'}]\"\"\",\n",
    "  \"output\": \"1\"},\n",
    "  {\n",
    "      \"query\":\"deep learning for code area, like coding, program, software, etc.\",\n",
    "      \"input\":\"\"\"http://127.0.0.1:40500/search?search=code&threshold=0.0&debug=True\n",
    "[{'abstract': 'The study explores the synergistic combination of Synthetic Aperture Radar\\n(SAR) and Visible-Near Infrared-Short Wave Infrared (VNIR-SWIR) imageries for\\nland use/land cover (LULC) classification. Image fusion, employing Bayesian\\nfusion, merges SAR texture bands with VNIR-SWIR imageries. The research aims to\\ninvestigate the impact of this fusion on LULC classification. Despite the\\npopularity of random forests for supervised classification, their limitations,\\nsuch as suboptimal performance with fewer features and accuracy stagnation, are\\naddressed. To overcome these issues, ensembles of random forests (RFE) are\\ncreated, introducing random rotations using the Forest-RC algorithm. Three\\nrotation approaches: principal component analysis (PCA), sparse random rotation\\n(SRP) matrix, and complete random rotation (CRP) matrix are employed.\\nSentinel-1 SAR data and Sentinel-2 VNIR-SWIR data from the IIT-Kanpur region\\nconstitute the training datasets, including SAR, SAR with texture, VNIR-SWIR,\\nVNIR-SWIR with texture, and fused VNIR-SWIR with texture. The study evaluates\\nclassifier efficacy, explores the impact of SAR and VNIR-SWIR fusion on\\nclassification, and significantly enhances the execution speed of Bayesian\\nfusion code. The SRP-based RFE outperforms other ensembles for the first two\\ndatasets, yielding average overall kappa values of 61.80% and 68.18%, while the\\nCRP-based RFE excels for the last three datasets with average overall kappa\\nvalues of 95.99%, 96.93%, and 96.30%. The fourth dataset achieves the highest\\noverall kappa of 96.93%. Furthermore, incorporating texture with SAR bands\\nresults in a maximum overall kappa increment of 10.00%, while adding texture to\\nVNIR-SWIR bands yields a maximum increment of approximately 3.45%.',\n",
    "  'arxiv_category': 'cs.CV, cs.AI, eess.IV',\n",
    "  'arxiv_id': '2312.10798',\n",
    "  'arxiv_upload_date': '2023-12-17T19:22:39',\n",
    "  'arxiv_url': 'https://arxiv.org/abs/2312.10798',\n",
    "  'authors': 'Shivam Pande',\n",
    "  'id': 30051,\n",
    "  'publication_date': None,\n",
    "  'publication_name': None,\n",
    "  'publication_url': None,\n",
    "  'title': 'Land use/land cover classification of fused Sentinel-1 and Sentinel-2   imageries using ensembles of Random Forests'},\n",
    " {'abstract': 'Symbolic music datasets are important for music information retrieval and\\nmusical analysis. However, there is a lack of large-scale symbolic datasets for\\nclassical piano music. In this article, we create a GiantMIDI-Piano (GP)\\ndataset containing 38,700,838 transcribed notes and 10,855 unique solo piano\\nworks composed by 2,786 composers. We extract the names of music works and the\\nnames of composers from the International Music Score Library Project (IMSLP).\\nWe search and download their corresponding audio recordings from the internet.\\nWe further create a curated subset containing 7,236 works composed by 1,787\\ncomposers by constraining the titles of downloaded audio recordings containing\\nthe surnames of composers. We apply a convolutional neural network to detect\\nsolo piano works. Then, we transcribe those solo piano recordings into Musical\\nInstrument Digital Interface (MIDI) files using a high-resolution piano\\ntranscription system. Each transcribed MIDI file contains the onset, offset,\\npitch, and velocity attributes of piano notes and pedals. GiantMIDI-Piano\\nincludes 90% live performance MIDI files and 10\\\\% sequence input MIDI files. We\\nanalyse the statistics of GiantMIDI-Piano and show pitch class, interval,\\ntrichord, and tetrachord frequencies of six composers from different eras to\\nshow that GiantMIDI-Piano can be used for musical analysis. We evaluate the\\nquality of GiantMIDI-Piano in terms of solo piano detection F1 scores, metadata\\naccuracy, and transcription error rates. We release the source code for\\nacquiring the GiantMIDI-Piano dataset at\\nhttps://github.com/bytedance/GiantMIDI-Piano',\n",
    "  'arxiv_category': 'cs.IR, cs.SD, eess.AS',\n",
    "  'arxiv_id': '2010.07061',\n",
    "  'arxiv_upload_date': '2020-10-11T01:23:43',\n",
    "  'arxiv_url': 'https://arxiv.org/abs/2010.07061',\n",
    "  'authors': 'Qiuqiang Kong, Bochen Li, Jitong Chen, Yuxuan Wang',\n",
    "  'id': 6752,\n",
    "  'publication_date': None,\n",
    "  'publication_name': None,\n",
    "  'publication_url': None,\n",
    "  'title': 'GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music'},\n",
    " {'abstract': \"Deep neural networks (DNNs) have advanced many machine learning tasks, but\\ntheir performance is often harmed by noisy labels in real-world data.\\nAddressing this, we introduce CoLafier, a novel approach that uses Local\\nIntrinsic Dimensionality (LID) for learning with noisy labels. CoLafier\\nconsists of two subnets: LID-dis and LID-gen. LID-dis is a specialized\\nclassifier. Trained with our uniquely crafted scheme, LID-dis consumes both a\\nsample's features and its label to predict the label - which allows it to\\nproduce an enhanced internal representation. We observe that LID scores\\ncomputed from this representation effectively distinguish between correct and\\nincorrect labels across various noise scenarios. In contrast to LID-dis,\\nLID-gen, functioning as a regular classifier, operates solely on the sample's\\nfeatures. During training, CoLafier utilizes two augmented views per instance\\nto feed both subnets. CoLafier considers the LID scores from the two views as\\nproduced by LID-dis to assign weights in an adapted loss function for both\\nsubnets. Concurrently, LID-gen, serving as classifier, suggests pseudo-labels.\\nLID-dis then processes these pseudo-labels along with two views to derive LID\\nscores. Finally, these LID scores along with the differences in predictions\\nfrom the two subnets guide the label update decisions. This dual-view and\\ndual-subnet approach enhances the overall reliability of the framework. Upon\\ncompletion of the training, we deploy the LID-gen subnet of CoLafier as the\\nfinal classification model. CoLafier demonstrates improved prediction accuracy,\\nsurpassing existing methods, particularly under severe label noise. For more\\ndetails, see the code at https://github.com/zdy93/CoLafier.\",\n",
    "  'arxiv_category': 'cs.LG, cs.AI',\n",
    "  'arxiv_id': '2401.05458',\n",
    "  'arxiv_upload_date': '2024-01-10T08:10:59',\n",
    "  'arxiv_url': 'https://arxiv.org/abs/2401.05458',\n",
    "  'authors': 'Dongyu Zhang, Ruofan Hu, Elke Rundensteiner',\n",
    "  'id': 35812,\n",
    "  'publication_date': None,\n",
    "  'publication_name': None,\n",
    "  'publication_url': None,\n",
    "  'title': 'CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic   Dimensionality Guidance'},\n",
    " {'abstract': 'Context: Comprehensive studies of Wolf-Rayet stars were performed in the past\\nfor the Galactic and the LMC population. The results revealed significant\\ndifferences, but also unexpected similarities between the WR populations of\\nthese different galaxies. Analyzing the WR stars in M31 will extend our\\nunderstanding of these objects in different galactic environments. Aims: The\\npresent study aims at the late-type WN stars in M31. The stellar and wind\\nparameters will tell about the formation of WR stars in other galaxies with\\ndifferent metallicity and star formation histories. The obtained parameters\\nwill provide constraints to the evolution of massive stars in the environment\\nof M31. Methods: We used the latest version of the Potsdam Wolf-Rayet model\\natmosphere code to analyze the stars via fitting optical spectra and\\nphotometric data. To account for the relatively low temperatures of the late\\nWN10 and WN11 subtypes, our WN models have been extended into this temperature\\nregime. Results: Stellar and atmospheric parameters are derived for all known\\nlate-type WN stars in M31 with available spectra. All of these stars still have\\nhydrogen in their outer envelopes, some of them up to 50% by mass. The stars\\nare located on the cool side of the zero age main sequence in the\\nHertzsprung-Russell diagram, while their luminosities range from $10^5$ to\\n$10^6$ Lsun. It is remarkable that no star exceeds $10^6$ Lsun. Conclusions: If\\nformed via single-star evolution, the late-type WN stars in M31 stem from an\\ninitial mass range between 20 and 60 Msun. From the very late-type WN9-11\\nstars, only one star is located in the S Doradus instability strip. We do not\\nfind any late-type WN stars with the high luminosities known in the Milky Way.',\n",
    "  'arxiv_category': 'astro-ph.SR, astro-ph.GA',\n",
    "  'arxiv_id': '1402.2282',\n",
    "  'arxiv_upload_date': '2014-02-10T21:00:21',\n",
    "  'arxiv_url': 'https://arxiv.org/abs/1402.2282',\n",
    "  'authors': 'Andreas Sander, Helge Todt, Rainer Hainich, Wolf-Rainer Hamann',\n",
    "  'id': 27318,\n",
    "  'publication_date': None,\n",
    "  'publication_name': None,\n",
    "  'publication_url': None,\n",
    "  'title': 'The Wolf-Rayet stars in M31: I. Analysis of the late-type WN stars'}]\"\"\",\n",
    "  \"output\":\"0\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{query} {input}\"),\n",
    "        (\"ai\",\"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = example,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. You will get an input and a list of papers and you need to tell me if they are relevant to my query. Only return 1 if the paper is relevant, otherwise return 0.\"),\n",
    "        #few_shot_prompt,\n",
    "        (\"human\", \"{input} {papers}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"all_proxy\"]=\"socks5://127.0.0.1:7891/\"\n",
    "os.environ[\"http_proxy\"]=\"socks5://127.0.0.1:7891/\"\n",
    "os.environ[\"https_proxy\"]=\"socks5://127.0.0.1:7891/\"\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\")\n",
    "chain = final_prompt | model | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"input\":\"Deep learning for code area, like coding, programing, etc. It show be about deep learning\",\n",
    "    \"papers\":papers[9:],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'abstract': \"Memory isolation is critical for system reliability, security, and safety.\\nUnfortunately, read disturbance can break memory isolation in modern DRAM\\nchips. For example, RowHammer is a well-studied read-disturb phenomenon where\\nrepeatedly opening and closing (i.e., hammering) a DRAM row many times causes\\nbitflips in physically nearby rows.\\n  This paper experimentally demonstrates and analyzes another widespread\\nread-disturb phenomenon, RowPress, in real DDR4 DRAM chips. RowPress breaks\\nmemory isolation by keeping a DRAM row open for a long period of time, which\\ndisturbs physically nearby rows enough to cause bitflips. We show that RowPress\\namplifies DRAM's vulnerability to read-disturb attacks by significantly\\nreducing the number of row activations needed to induce a bitflip by one to two\\norders of magnitude under realistic conditions. In extreme cases, RowPress\\ninduces bitflips in a DRAM row when an adjacent row is activated only once. Our\\ndetailed characterization of 164 real DDR4 DRAM chips shows that RowPress 1)\\naffects chips from all three major DRAM manufacturers, 2) gets worse as DRAM\\ntechnology scales down to smaller node sizes, and 3) affects a different set of\\nDRAM cells from RowHammer and behaves differently from RowHammer as temperature\\nand access pattern changes.\\n  We demonstrate in a real DDR4-based system with RowHammer protection that 1)\\na user-level program induces bitflips by leveraging RowPress while conventional\\nRowHammer cannot do so, and 2) a memory controller that adaptively keeps the\\nDRAM row open for a longer period of time based on access pattern can\\nfacilitate RowPress-based attacks. To prevent bitflips due to RowPress, we\\ndescribe and evaluate a new methodology that adapts existing RowHammer\\nmitigation techniques to also mitigate RowPress with low additional performance\\noverhead. We open source all our code and data to facilitate future research on\\nRowPress.\",\n",
       "  'arxiv_category': 'cs.CR, cs.AR',\n",
       "  'arxiv_id': '2306.17061',\n",
       "  'arxiv_upload_date': '2023-06-29T16:09:56',\n",
       "  'arxiv_url': 'https://arxiv.org/abs/2306.17061',\n",
       "  'authors': 'Haocong Luo, Ataberk Olgun, A. Giray Yağlıkçı, Yahya Can Tuğrul, Steve Rhyner, Meryem Banu Cavlak, Joël Lindegger, Mohammad Sadrosadati, Onur Mutlu',\n",
       "  'id': 35500,\n",
       "  'publication_date': None,\n",
       "  'publication_name': None,\n",
       "  'publication_url': None,\n",
       "  'title': 'RowPress: Amplifying Read Disturbance in Modern DRAM Chips'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_search(search, threshold=0.0, debug=True):\n",
    "    # 设置查询参数\n",
    "    params = {\n",
    "        'search': search,\n",
    "        'threshold': threshold,  # 设置相似度阈值\n",
    "        'debug': debug   # 设置是否开启调试模式\n",
    "    }\n",
    "\n",
    "    # 构建查询字符串\n",
    "    query_string = urllib.parse.urlencode(params)\n",
    "    url = f\"http://127.0.0.1:40500/search?{query_string}\"\n",
    "\n",
    "    # 发送 GET 请求\n",
    "    response = requests.get(url)\n",
    "    # 检查响应状态并处理\n",
    "    if response.status_code == 200:\n",
    "        papers = response.json()\n",
    "        return papers\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(threshold)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m papers \u001b[39m=\u001b[39m paper_search(search, threshold, debug \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m response \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49minvoke({\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m:query,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mpapers\u001b[39;49m\u001b[39m\"\u001b[39;49m:papers[\u001b[39m9\u001b[39;49m:],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m })\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m response \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:1774\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1774\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1775\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1776\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1777\u001b[0m             patch_config(\n\u001b[1;32m   1778\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1779\u001b[0m             ),\n\u001b[1;32m   1780\u001b[0m         )\n\u001b[1;32m   1781\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:3074\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3072\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfunc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 3074\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[1;32m   3075\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invoke,\n\u001b[1;32m   3076\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   3077\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config(config, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc),\n\u001b[1;32m   3078\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3079\u001b[0m     )\n\u001b[1;32m   3080\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3081\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   3082\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3083\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse `ainvoke` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3084\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:975\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     context \u001b[39m=\u001b[39m copy_context()\n\u001b[1;32m    972\u001b[0m     context\u001b[39m.\u001b[39mrun(var_child_runnable_config\u001b[39m.\u001b[39mset, child_config)\n\u001b[1;32m    973\u001b[0m     output \u001b[39m=\u001b[39m cast(\n\u001b[1;32m    974\u001b[0m         Output,\n\u001b[0;32m--> 975\u001b[0m         context\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    976\u001b[0m             call_func_with_variable_args,\n\u001b[1;32m    977\u001b[0m             func,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    978\u001b[0m             \u001b[39minput\u001b[39;49m,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    979\u001b[0m             config,\n\u001b[1;32m    980\u001b[0m             run_manager,\n\u001b[1;32m    981\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    982\u001b[0m         ),\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    984\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    985\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/config.py:323\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    322\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 323\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:2950\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   2948\u001b[0m                 output \u001b[39m=\u001b[39m chunk\n\u001b[1;32m   2949\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2950\u001b[0m     output \u001b[39m=\u001b[39m call_func_with_variable_args(\n\u001b[1;32m   2951\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39minput\u001b[39;49m, config, run_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   2952\u001b[0m     )\n\u001b[1;32m   2953\u001b[0m \u001b[39m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   2954\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/config.py:323\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    322\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 323\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb 单元格 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mschema\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrunnable\u001b[39;00m \u001b[39mimport\u001b[39;00m RunnableLambda\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m prep \u001b[39m=\u001b[39m RunnableLambda(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x:[{\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m:doc} \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m divide_list(x, \u001b[39m5\u001b[39;49m)],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "\u001b[1;32m/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# 计算每个子列表的长度，使用整除和取模来处理不均等的情况\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m q, r \u001b[39m=\u001b[39m \u001b[39mdivmod\u001b[39m(\u001b[39mlen\u001b[39m(lst), n)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [lst[i\u001b[39m*\u001b[39;49mq \u001b[39m+\u001b[39;49m \u001b[39mmin\u001b[39;49m(i, r):(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m*\u001b[39;49mq \u001b[39m+\u001b[39;49m \u001b[39mmin\u001b[39;49m(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, r)] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n)]\n",
      "\u001b[1;32m/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# 计算每个子列表的长度，使用整除和取模来处理不均等的情况\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m q, r \u001b[39m=\u001b[39m \u001b[39mdivmod\u001b[39m(\u001b[39mlen\u001b[39m(lst), n)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m [lst[i\u001b[39m*\u001b[39;49mq \u001b[39m+\u001b[39;49m \u001b[39mmin\u001b[39;49m(i, r):(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m*\u001b[39;49mq \u001b[39m+\u001b[39;49m \u001b[39mmin\u001b[39;49m(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, r)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n)]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "flag = 1\n",
    "query = \"Deep learning for code area, like coding, programing, etc. It should be about deep learning\"\n",
    "search = \"code\"\n",
    "threshold = 0.1\n",
    "last_response = None\n",
    "last_threshold = None\n",
    "while flag:\n",
    "    print(threshold)\n",
    "    papers = paper_search(search, threshold, debug = True)\n",
    "    response = chain.invoke({\n",
    "        \"input\":query,\n",
    "        \"papers\":papers[9:],\n",
    "    })\n",
    "    print(response)\n",
    "    if response not in [0,1]:\n",
    "        raise Exception(\"invalid respnse\")\n",
    "    \n",
    "    if papers == [] and last_response == None:\n",
    "        raise Exception(\"no papers found\")\n",
    "    elif papers == [] and last_response != None:\n",
    "        results = paper_search(search, last_threshold, debug = False)\n",
    "\n",
    "    if last_response is not None and response != last_response:\n",
    "        print(\"over\")\n",
    "        results = paper_search(search, threshold, debug = False)\n",
    "        break\n",
    "    last_threshold = threshold\n",
    "    if response == 0:\n",
    "        print(\"threshold + 0.05\")\n",
    "        threshold += 0.05\n",
    "    elif response == 1:\n",
    "        print(\"threshold - 0.05\")\n",
    "        threshold -= 0.05\n",
    "    last_response = response\n",
    "    if threshold < 0:\n",
    "        threshold = 0\n",
    "        results = paper_search(search, threshold, debug = False)\n",
    "        break\n",
    "    if threshold > 1:\n",
    "        raise Exception(\"threshold > 1\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 'Large Language Models (LLMs) have shown remarkable capabilities in processing\\nboth natural and programming languages, which have enabled various applications\\nin software engineering, such as requirement engineering, code generation, and\\nsoftware testing. However, existing code generation benchmarks do not\\nnecessarily assess the code understanding performance of LLMs, especially for\\nthe subtle inconsistencies that may arise between code and its semantics\\ndescribed in natural language.\\n  In this paper, we propose a novel method to systematically assess the code\\nunderstanding performance of LLMs, particularly focusing on subtle differences\\nbetween code and its descriptions, by introducing code mutations to existing\\ncode generation datasets. Code mutations are small changes that alter the\\nsemantics of the original code, creating a mismatch with the natural language\\ndescription. We apply different types of code mutations, such as operator\\nreplacement and statement deletion, to generate inconsistent code-description\\npairs. We then use these pairs to test the ability of LLMs to correctly detect\\nthe inconsistencies.\\n  We propose a new LLM testing method, called Mutation-based Consistency\\nTesting (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and\\nGPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which\\nconsists of six programming languages (Python, C++, Java, Go, JavaScript, and\\nRust). We compare the performance of the LLMs across different types of code\\nmutations and programming languages and analyze the results. We find that the\\nLLMs show significant variation in their code understanding performance and\\nthat they have different strengths and weaknesses depending on the mutation\\ntype and language.',\n",
       " 'arxiv_category': 'cs.SE, cs.AI',\n",
       " 'arxiv_id': '2401.05940',\n",
       " 'arxiv_upload_date': '2024-01-11T14:27:43',\n",
       " 'arxiv_url': 'https://arxiv.org/abs/2401.05940',\n",
       " 'authors': 'Ziyu Li, Donghwan Shin',\n",
       " 'id': 35598,\n",
       " 'publication_date': None,\n",
       " 'publication_name': None,\n",
       " 'publication_url': None,\n",
       " 'title': 'Mutation-based Consistency Testing for Evaluating the Code Understanding   Capability of LLMs'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.v1 import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "class Keywords(BaseModel):\n",
    "    \"\"\"Keywords for a paper\"\"\"\n",
    "    keywords: str = Field(..., title=\"Keywords\", description=\"Keywords for a paper\")\n",
    "convert_pydantic_to_openai_function(Keywords)\n",
    "extraction_functions = [convert_pydantic_to_openai_function(Keywords)]\n",
    "extraction_model = model.bind(functions = extraction_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"keywords\": \"large language models, code understanding, code generation, benchmark, code mutations, inconsistency detection, LLM testing\"\\n}', 'name': 'Keywords'}})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "extraction_model.invoke(json.dumps(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'papers': [{'keywords': 'Large Language Models, code understanding, code generation, code mutations, inconsistencies, LLM testing, programming languages'},\n",
       "  {'keywords': 'LLMs, code generation, sub-functions, code completion, zero-shot evaluation, code editing instructions'},\n",
       "  {'keywords': 'code intelligence, deep learning, code representation learning, benchmark, neural models, code intelligence models'},\n",
       "  {'keywords': 'code representation learning, abstract syntax tree, code-related tasks, AST-based code representation'},\n",
       "  {'keywords': 'LLMs, code-specific adversarial attacks, code structure, transferability, programming languages'},\n",
       "  {'keywords': 'LLMs, code editing instructions, benchmark, open and closed models, code edits, fine-tuning'},\n",
       "  {'keywords': 'LLMs, code vulnerability repair, reinforcement learning, semantic reward, code comments'},\n",
       "  {'keywords': 'pre-trained models, source code, probing tasks, surface characteristics, syntactic characteristics, structural characteristics, semantic characteristics'},\n",
       "  {'keywords': 'LLMs, feedback-driven solution synthesis, security vulnerabilities, code refinement, PythonSecurityEval dataset'},\n",
       "  {'keywords': 'backdoor attacks, neural code models, triggers, task-agnostic attacks, code understanding, code generation'}]}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Extract the keywords from the paper. Extract as detailed as possible and exclude irrelevant keywords. Do not make up or guess ANY extra information.\"),\n",
    "    (\"human\",\"{input}\"),\n",
    "])\n",
    "class Info(BaseModel):\n",
    "    \"\"\"Information to extract\"\"\"\n",
    "    papers: List[Keywords]\n",
    "paper_extraction_function = [convert_pydantic_to_openai_function(Info)]\n",
    "paper_extraction_model = model.bind(functions = paper_extraction_function, function_call={\"name\":\"Info\"})\n",
    "paper_extraction_chain = prompt | paper_extraction_model | JsonOutputFunctionsParser()\n",
    "\n",
    "paper_extraction_chain.invoke({\"input\": \" \".join(json.dumps(paper) for paper in results[0:10])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19053\n"
     ]
    }
   ],
   "source": [
    "def divide_list(lst, n):\n",
    "    return [\" \".join(json.dumps(paper) for paper in lst[i:i + n])for i in range(0, len(lst), n)]\n",
    "\n",
    "a = divide_list(results, 10)\n",
    "print(len(a[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"abstract\": \"Large Language Models (LLMs) have shown remarkable capabilities in processing\\nboth natural and programming languages, which have enabled various applications\\nin software engineering, such as requirement engineering, code generation, and\\nsoftware testing. However, existing code generation benchmarks do not\\nnecessarily assess the code understanding performance of LLMs, especially for\\nthe subtle inconsistencies that may arise between code and its semantics\\ndescribed in natural language.\\n  In this paper, we propose a novel method to systematically assess the code\\nunderstanding performance of LLMs, particularly focusing on subtle differences\\nbetween code and its descriptions, by introducing code mutations to existing\\ncode generation datasets. Code mutations are small changes that alter the\\nsemantics of the original code, creating a mismatch with the natural language\\ndescription. We apply different types of code mutations, such as operator\\nreplacement and statement deletion, to generate inconsistent code-description\\npairs. We then use these pairs to test the ability of LLMs to correctly detect\\nthe inconsistencies.\\n  We propose a new LLM testing method, called Mutation-based Consistency\\nTesting (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and\\nGPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which\\nconsists of six programming languages (Python, C++, Java, Go, JavaScript, and\\nRust). We compare the performance of the LLMs across different types of code\\nmutations and programming languages and analyze the results. We find that the\\nLLMs show significant variation in their code understanding performance and\\nthat they have different strengths and weaknesses depending on the mutation\\ntype and language.\", \"arxiv_category\": \"cs.SE, cs.AI\", \"arxiv_id\": \"2401.05940\", \"arxiv_upload_date\": \"2024-01-11T14:27:43\", \"arxiv_url\": \"https://arxiv.org/abs/2401.05940\", \"authors\": \"Ziyu Li, Donghwan Shin\", \"id\": 35598, \"publication_date\": null, \"publication_name\": null, \"publication_url\": null, \"title\": \"Mutation-based Consistency Testing for Evaluating the Code Understanding   Capability of LLMs\"} {\"abstract\": \"This work introduces (1) a technique that allows large language models (LLMs)\\nto leverage user-provided code when solving programming tasks and (2) a method\\nto iteratively generate modular sub-functions that can aid future code\\ngeneration attempts when the initial code generated by the LLM is inadequate.\\nGenerating computer programs in general-purpose programming languages like\\nPython poses a challenge for LLMs when instructed to use code provided in the\\nprompt. Code-specific LLMs (e.g., GitHub Copilot, CodeLlama2) can generate code\\ncompletions in real-time by drawing on all code available in a development\\nenvironment. However, restricting code-specific LLMs to use only in-context\\ncode is not straightforward, as the model is not explicitly instructed to use\\nthe user-provided code and users cannot highlight precisely which snippets of\\ncode the model should incorporate into its context. Moreover, current systems\\nlack effective recovery methods, forcing users to iteratively re-prompt the\\nmodel with modified prompts until a sufficient solution is reached. Our method\\ndiffers from traditional LLM-powered code-generation by constraining\\ncode-generation to an explicit function set and enabling recovery from failed\\nattempts through automatically generated sub-functions. When the LLM cannot\\nproduce working code, we generate modular sub-functions to aid subsequent\\nattempts at generating functional code. A by-product of our method is a library\\nof reusable sub-functions that can solve related tasks, imitating a software\\nteam where efficiency scales with experience. We also introduce a new\\n\\\"half-shot\\\" evaluation paradigm that provides tighter estimates of LLMs' coding\\nabilities compared to traditional zero-shot evaluation. Our proposed evaluation\\nmethod encourages models to output solutions in a structured format, decreasing\\nsyntax errors that can be mistaken for poor coding ability.\", \"arxiv_category\": \"cs.LG, cs.CL, cs.PL\", \"arxiv_id\": \"2311.15500\", \"arxiv_upload_date\": \"2023-11-27T02:55:34\", \"arxiv_url\": \"https://arxiv.org/abs/2311.15500\", \"authors\": \"Patrick Hajali, Ignas Budvytis\", \"id\": 25197, \"publication_date\": null, \"publication_name\": null, \"publication_url\": null, \"title\": \"Function-constrained Program Synthesis\"} {\"abstract\": \"Code intelligence leverages machine learning techniques to extract knowledge\\nfrom extensive code corpora, with the aim of developing intelligent tools to\\nimprove the quality and productivity of computer programming. Currently, there\\nis already a thriving research community focusing on code intelligence, with\\nefforts ranging from software engineering, machine learning, data mining,\\nnatural language processing, and programming languages. In this paper, we\\nconduct a comprehensive literature review on deep learning for code\\nintelligence, from the aspects of code representation learning, deep learning\\ntechniques, and application tasks. We also benchmark several state-of-the-art\\nneural models for code intelligence, and provide an open-source toolkit\\ntailored for the rapid prototyping of deep-learning-based code intelligence\\nmodels. In particular, we inspect the existing code intelligence models under\\nthe basis of code representation learning, and provide a comprehensive overview\\nto enhance comprehension of the present state of code intelligence.\\nFurthermore, we publicly release the source code and data resources to provide\\nthe community with a ready-to-use benchmark, which can facilitate the\\nevaluation and comparison of existing and future code intelligence models\\n(https://xcodemind.github.io). At last, we also point out several challenging\\nand promising directions for future research.\", \"arxiv_category\": \"cs.SE, cs.AI\", \"arxiv_id\": \"2401.00288\", \"arxiv_upload_date\": \"2023-12-30T17:48:37\", \"arxiv_url\": \"https://arxiv.org/abs/2401.00288\", \"authors\": \"Yao Wan, Yang He, Zhangqian Bi, Jianguo Zhang, Hongyu Zhang, Yulei Sui, Guandong Xu, Hai Jin, Philip S. Yu\", \"id\": 33504, \"publication_date\": null, \"publication_name\": null, \"publication_url\": null, \"title\": \"Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit\"} {\"abstract\": \"Programming language understanding and representation (a.k.a code\\nrepresentation learning) has always been a hot and challenging task in software\\nengineering. It aims to apply deep learning techniques to produce numerical\\nrepresentations of the source code features while preserving its semantics.\\nThese representations can be used for facilitating subsequent code-related\\ntasks. The abstract syntax tree (AST), a fundamental code feature, illustrates\\nthe syntactic information of the source code and has been widely used in code\\nrepresentation learning. However, there is still a lack of systematic and\\nquantitative evaluation of how well AST-based code representation facilitates\\nsubsequent code-related tasks. In this paper, we first conduct a comprehensive\\nempirical study to explore the effectiveness of the AST-based code\\nrepresentation in facilitating follow-up code-related tasks. To do so, we\\ncompare the performance of models trained with code token sequence (Token for\\nshort) based code representation and AST-based code representation on three\\npopular types of code-related tasks. Surprisingly, the overall quantitative\\nstatistical results demonstrate that models trained with AST-based code\\nrepresentation consistently perform worse across all three tasks compared to\\nmodels trained with Token-based code representation. Our further quantitative\\nanalysis reveals that models trained with AST-based code representation\\noutperform models trained with Token-based code representation in certain\\nsubsets of samples across all three tasks. We also conduct comprehensive\\nexperiments to evaluate and reveal the impact of the choice of AST\\nparsing/preprocessing/encoding methods on AST-based code representation and\\nsubsequent code-related tasks. Our study provides future researchers with\\ndetailed guidance on how to select solutions at each stage to fully exploit\\nAST.\", \"arxiv_category\": \"cs.SE, cs.AI, cs.CL, cs.PL, 68-04, 68T30, D.2.3; I.2.2; I.2.4\", \"arxiv_id\": \"2312.00413\", \"arxiv_upload_date\": \"2023-12-01T08:37:27\", \"arxiv_url\": \"https://arxiv.org/abs/2312.00413\", \"authors\": \"Weisong Sun, Chunrong Fang, Yun Miao, Yudu You, Mengzhe Yuan, Yuchen Chen, Quanjun Zhang, An Guo, Xiang Chen, Yang Liu, Zhenyu Chen\", \"id\": 24039, \"publication_date\": null, \"publication_name\": null, \"publication_url\": null, \"title\": \"Abstract Syntax Tree for Programming Language Understanding and   Representation: How Far Are We?\"} {\"abstract\": \"Pre-trained programming language (PL) models (such as CodeT5, CodeBERT,\\nGraphCodeBERT, etc.,) have the potential to automate software engineering tasks\\ninvolving code understanding and code generation. However, these models operate\\nin the natural channel of code, i.e., they are primarily concerned with the\\nhuman understanding of the code. They are not robust to changes in the input\\nand thus, are potentially susceptible to adversarial attacks in the natural\\nchannel. We propose, CodeAttack, a simple yet effective black-box attack model\\nthat uses code structure to generate effective, efficient, and imperceptible\\nadversarial code samples and demonstrates the vulnerabilities of the\\nstate-of-the-art PL models to code-specific adversarial attacks. We evaluate\\nthe transferability of CodeAttack on several code-code (translation and repair)\\nand code-NL (summarization) tasks across different programming languages.\\nCodeAttack outperforms state-of-the-art adversarial NLP attack models to\\nachieve the best overall drop in performance while being more efficient,\\nimperceptible, consistent, and fluent. The code can be found at\\nhttps://github.com/reddy-lab-code-research/CodeAttack.\", \"arxiv_category\": \"cs.CL, cs.CR\", \"arxiv_id\": \"2206.00052\", \"arxiv_upload_date\": \"2022-05-31T18:40:01\", \"arxiv_url\": \"https://arxiv.org/abs/2206.00052\", \"authors\": \"Akshita Jha, Chandan K. Reddy\", \"id\": 12698, \"publication_date\": null, \"publication_name\": null, \"publication_url\": null, \"title\": \"CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming   Language Models\"} {\"abstract\": \"A significant amount of research is focused on developing and evaluating\\nlarge language models for a variety of code synthesis tasks. These include\\nsynthesizing code from natural language instructions, synthesizing tests from\\ncode, and synthesizing explanations of code. In contrast, the behavior of\\ninstructional code editing with LLMs is understudied. These are tasks in which\\nthe model is instructed to update a block of code provided in a prompt. The\\nediting instruction may ask for a feature to added or removed, describe a bug\\nand ask for a fix, ask for a different kind of solution, or many other common\\ncode editing tasks.\\n  We introduce a carefully crafted benchmark of code editing tasks and use it\\nevaluate several cutting edge LLMs. Our evaluation exposes a significant gap\\nbetween the capabilities of state-of-the-art open and closed models. For\\nexample, even GPT-3.5-Turbo is 8.8% better than the best open model at editing\\ncode.\\n  We also introduce a new, carefully curated, permissively licensed training\\nset of code edits coupled with natural language instructions. Using this\\ntraining set, we show that we can fine-tune open Code LLMs to significantly\\nimprove their code editing capabilities.\", \"arxiv_category\": \"cs.SE, cs.AI, cs.LG, cs.PL\", \"arxiv_id\": \"2312.12450\", \"arxiv_upload_date\": \"2023-12-11T02:27:45\", \"arxiv_url\": \"https://arxiv.org/abs/2312.12450\", \"authors\": \"Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Anton Lozhkov, Carolyn Jane Anderson, Arjun Guha\", \"id\": 33073, \"publication_date\": null, \"publication_name\": null, \"publication_url\": null, \"title\": \"Can It Edit? Evaluating the Ability of Large Language Models to Follow   Code Editing Instructions\"} {\"abstract\": \"In software development, the predominant emphasis on functionality often\\nsupersedes security concerns, a trend gaining momentum with AI-driven\\nautomation tools like GitHub Copilot. These tools significantly improve\\ndevelopers' efficiency in functional code development. Nevertheless, it remains\\na notable concern that such tools are also responsible for creating insecure\\ncode, predominantly because of pre-training on publicly available repositories\\nwith vulnerable code. Moreover, developers are called the \\\"weakest link in the\\nchain\\\" since they have very minimal knowledge of code security. Although\\nexisting solutions provide a reasonable solution to vulnerable code, they must\\nadequately describe and educate the developers on code security to ensure that\\nthe security issues are not repeated. Therefore we introduce a multipurpose\\ncode vulnerability analysis system \\\\texttt{SecRepair}, powered by a large\\nlanguage model, CodeGen2 assisting the developer in identifying and generating\\nfixed code along with a complete description of the vulnerability with a code\\ncomment. Our innovative methodology uses a reinforcement learning paradigm to\\ngenerate code comments augmented by a semantic reward mechanism. Inspired by\\nhow humans fix code issues, we propose an instruction-based dataset suitable\\nfor vulnerability analysis with LLMs. We further identify zero-day and N-day\\nvulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findings\\nunderscore that incorporating reinforcement learning coupled with semantic\\nreward augments our model's performance, thereby fortifying its capacity to\\naddress code vulnerabilities with improved efficacy.\", \"arxiv_category\": \"cs.SE, cs.AI\", \"arxiv_id\": \"2401.03374\", \"arxiv_upload_date\": \"2024-01-07T02:46:39\", \"arxiv_url\": \"https://arxiv.org/abs/2401.03374\", \"authors\": \"Nafis Tanveer Islam, Joseph Khoury, Andrew Seong, Gonzalo De La Torre Parra, Elias Bou-Harb, Peyman Najafirad\", \"id\": 34719, \"publication_date\": null, \"publication_name\": null, \"publication_url\": null, \"title\": \"LLM-Powered Code Vulnerability Repair with Reinforcement Learning and   Semantic Reward\"} {\"abstract\": \"Pre-trained models of source code have recently been successfully applied to\\na wide variety of Software Engineering tasks; they have also seen some\\npractical adoption in practice, e.g. for code completion. Yet, we still know\\nvery little about what these pre-trained models learn about source code. In\\nthis article, we use probing--simple diagnostic tasks that do not further train\\nthe models--to discover to what extent pre-trained models learn about specific\\naspects of source code. We use an extensible framework to define 15 probing\\ntasks that exercise surface, syntactic, structural and semantic characteristics\\nof source code. We probe 8 pre-trained source code models, as well as a natural\\nlanguage model (BERT) as our baseline. We find that models that incorporate\\nsome structural information (such as GraphCodeBERT) have a better\\nrepresentation of source code characteristics. Surprisingly, we find that for\\nsome probing tasks, BERT is competitive with the source code models, indicating\\nthat there are ample opportunities to improve source-code specific pre-training\\non the respective code characteristics. We encourage other researchers to\\nevaluate their models with our probing task suite, so that they may peer into\\nthe hidden layers of the models and identify what intrinsic code\\ncharacteristics are encoded.\", \"arxiv_category\": \"cs.SE, cs.LG\", \"arxiv_id\": \"2312.05092\", \"arxiv_upload_date\": \"2023-12-08T15:21:54\", \"arxiv_url\": \"https://arxiv.org/abs/2312.05092\", \"authors\": \"Anjan Karmakar, Romain Robbes\", \"id\": 27033, \"publication_date\": null, \"publication_name\": null, \"publication_url\": null, \"title\": \"INSPECT: Intrinsic and Systematic Probing Evaluation for Code   Transformers\"} {\"abstract\": \"Large Language Models (LLMs) have shown impressive proficiency in code\\ngeneration. Nonetheless, similar to human developers, these models might\\ngenerate code that contains security vulnerabilities and flaws. Writing secure\\ncode remains a substantial challenge, as vulnerabilities often arise during\\ninteractions between programs and external systems or services, such as\\ndatabases and operating systems. In this paper, we propose a novel approach,\\nFeedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMs\\nin receiving feedback from Bandit, which is a static code analysis tool, and\\nthen the LLMs generate potential solutions to resolve security vulnerabilities.\\nEach solution, along with the vulnerable code, is then sent back to the LLM for\\ncode refinement. Our approach shows a significant improvement over the baseline\\nand outperforms existing approaches. Furthermore, we introduce a new dataset,\\nPythonSecurityEval, collected from real-world scenarios on Stack Overflow to\\nevaluate the LLMs' ability to generate secure code. Code and data are available\\nat \\\\url{https://github.com/Kamel773/LLM-code-refine}\", \"arxiv_category\": \"cs.CR, cs.LG\", \"arxiv_id\": \"2312.00024\", \"arxiv_upload_date\": \"2023-11-13T08:54:37\", \"arxiv_url\": \"https://arxiv.org/abs/2312.00024\", \"authors\": \"Kamel Alrashedy, Abdullah Aljasser\", \"id\": 26115, \"publication_date\": null, \"publication_name\": null, \"publication_url\": null, \"title\": \"Can LLMs Patch Security Issues?\"} {\"abstract\": \"Backdoor attacks for neural code models have gained considerable attention\\ndue to the advancement of code intelligence. However, most existing works\\ninsert triggers into task-specific data for code-related downstream tasks,\\nthereby limiting the scope of attacks. Moreover, the majority of attacks for\\npre-trained models are designed for understanding tasks. In this paper, we\\npropose task-agnostic backdoor attacks for code pre-trained models. Our\\nbackdoored model is pre-trained with two learning strategies (i.e., Poisoned\\nSeq2Seq learning and token representation learning) to support the multi-target\\nattack of downstream code understanding and generation tasks. During the\\ndeployment phase, the implanted backdoors in the victim models can be activated\\nby the designed triggers to achieve the targeted attack. We evaluate our\\napproach on two code understanding tasks and three code generation tasks over\\nseven datasets. Extensive experiments demonstrate that our approach can\\neffectively and stealthily attack code-related downstream tasks.\", \"arxiv_category\": \"cs.CR, cs.AI, cs.CL\", \"arxiv_id\": \"2306.08350\", \"arxiv_upload_date\": \"2023-06-14T08:38:51\", \"arxiv_url\": \"https://arxiv.org/abs/2306.08350\", \"authors\": \"Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang, Yang Liu\", \"id\": 11742, \"publication_date\": null, \"publication_name\": null, \"publication_url\": null, \"title\": \"Multi-target Backdoor Attacks for Code Pre-trained Models\"}\n"
     ]
    }
   ],
   "source": [
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "prep = RunnableLambda(\n",
    "    lambda x:[{\"input\":doc} for doc in divide_list(x, 5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prep | paper_extraction_chain.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-B4eFQ7o7oEW1hnR69YX2eHCc on tokens per min (TPM): Limit 80000, Used 77628, Requested 4752. Please try again in 1.785s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb 单元格 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hsw/Sync/papersearch/PaperHelper/Utils/keywords_extraction.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m chain\u001b[39m.\u001b[39;49minvoke(results)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:1774\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1774\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1775\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1776\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1777\u001b[0m             patch_config(\n\u001b[1;32m   1778\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1779\u001b[0m             ),\n\u001b[1;32m   1780\u001b[0m         )\n\u001b[1;32m   1781\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:3375\u001b[0m, in \u001b[0;36mRunnableEachBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3372\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m   3373\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: List[Input], config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m   3374\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Output]:\n\u001b[0;32m-> 3375\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_invoke, \u001b[39minput\u001b[39;49m, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:975\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     context \u001b[39m=\u001b[39m copy_context()\n\u001b[1;32m    972\u001b[0m     context\u001b[39m.\u001b[39mrun(var_child_runnable_config\u001b[39m.\u001b[39mset, child_config)\n\u001b[1;32m    973\u001b[0m     output \u001b[39m=\u001b[39m cast(\n\u001b[1;32m    974\u001b[0m         Output,\n\u001b[0;32m--> 975\u001b[0m         context\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    976\u001b[0m             call_func_with_variable_args,\n\u001b[1;32m    977\u001b[0m             func,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    978\u001b[0m             \u001b[39minput\u001b[39;49m,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    979\u001b[0m             config,\n\u001b[1;32m    980\u001b[0m             run_manager,\n\u001b[1;32m    981\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    982\u001b[0m         ),\n\u001b[1;32m    983\u001b[0m     )\n\u001b[1;32m    984\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    985\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/config.py:323\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mif\u001b[39;00m run_manager \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    322\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m run_manager\n\u001b[0;32m--> 323\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:3368\u001b[0m, in \u001b[0;36mRunnableEachBase._invoke\u001b[0;34m(self, inputs, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_invoke\u001b[39m(\n\u001b[1;32m   3362\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3363\u001b[0m     inputs: List[Input],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3366\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   3367\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Output]:\n\u001b[0;32m-> 3368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbound\u001b[39m.\u001b[39;49mbatch(\n\u001b[1;32m   3369\u001b[0m         inputs, patch_config(config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child()), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m   3370\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:1914\u001b[0m, in \u001b[0;36mRunnableSequence.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m   1912\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1913\u001b[0m         \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1914\u001b[0m             inputs \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49mbatch(\n\u001b[1;32m   1915\u001b[0m                 inputs,\n\u001b[1;32m   1916\u001b[0m                 [\n\u001b[1;32m   1917\u001b[0m                     \u001b[39m# each step a child run of the corresponding root run\u001b[39;49;00m\n\u001b[1;32m   1918\u001b[0m                     patch_config(\n\u001b[1;32m   1919\u001b[0m                         config, callbacks\u001b[39m=\u001b[39;49mrm\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1920\u001b[0m                     )\n\u001b[1;32m   1921\u001b[0m                     \u001b[39mfor\u001b[39;49;00m rm, config \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(run_managers, configs)\n\u001b[1;32m   1922\u001b[0m                 ],\n\u001b[1;32m   1923\u001b[0m             )\n\u001b[1;32m   1925\u001b[0m \u001b[39m# finish the root runs\u001b[39;00m\n\u001b[1;32m   1926\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:3629\u001b[0m, in \u001b[0;36mRunnableBindingBase.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m   3627\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3628\u001b[0m     configs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_configs(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(inputs))]\n\u001b[0;32m-> 3629\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbound\u001b[39m.\u001b[39;49mbatch(\n\u001b[1;32m   3630\u001b[0m     inputs,\n\u001b[1;32m   3631\u001b[0m     configs,\n\u001b[1;32m   3632\u001b[0m     return_exceptions\u001b[39m=\u001b[39;49mreturn_exceptions,\n\u001b[1;32m   3633\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs},\n\u001b[1;32m   3634\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:529\u001b[0m, in \u001b[0;36mRunnable.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(List[Output], [invoke(inputs[\u001b[39m0\u001b[39m], configs[\u001b[39m0\u001b[39m])])\n\u001b[1;32m    528\u001b[0m \u001b[39mwith\u001b[39;00m get_executor_for_config(configs[\u001b[39m0\u001b[39m]) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m--> 529\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(List[Output], \u001b[39mlist\u001b[39;49m(executor\u001b[39m.\u001b[39;49mmap(invoke, inputs, configs)))\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    457\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/config.py:442\u001b[0m, in \u001b[0;36mContextThreadPoolExecutor.map.<locals>._wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m contexts\u001b[39m.\u001b[39;49mpop()\u001b[39m.\u001b[39;49mrun(fn, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/runnables/base.py:522\u001b[0m, in \u001b[0;36mRunnable.batch.<locals>.invoke\u001b[0;34m(input, config)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[39mreturn\u001b[39;00m e\n\u001b[1;32m    521\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 522\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minvoke(\u001b[39minput\u001b[39;49m, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:165\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    156\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    161\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m    162\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    164\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 165\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    166\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    167\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    168\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    169\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    170\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    171\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    172\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    173\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m    174\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:543\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    536\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    537\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    541\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    542\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 543\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:407\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    406\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 407\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    408\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    409\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    410\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    411\u001b[0m ]\n\u001b[1;32m    412\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:397\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    395\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    396\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 397\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    398\u001b[0m                 m,\n\u001b[1;32m    399\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    400\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    401\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    402\u001b[0m             )\n\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:576\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    573\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    574\u001b[0m     )\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 576\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    577\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:437\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    432\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m    433\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    434\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream} \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[1;32m    435\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    436\u001b[0m }\n\u001b[0;32m--> 437\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(messages\u001b[39m=\u001b[39;49mmessage_dicts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    438\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/openai/resources/chat/completions.py:643\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    595\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    596\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    642\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 643\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    644\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    645\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    646\u001b[0m             {\n\u001b[1;32m    647\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    648\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    649\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    650\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    651\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    652\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    653\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    654\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    655\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    656\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    657\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    658\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    659\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    660\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    661\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    662\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    663\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    664\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    665\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    666\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    667\u001b[0m             },\n\u001b[1;32m    668\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    669\u001b[0m         ),\n\u001b[1;32m    670\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    671\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    672\u001b[0m         ),\n\u001b[1;32m    673\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    674\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    675\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    676\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/openai/_base_client.py:1112\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1099\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1100\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1108\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1109\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1110\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[0;32m-> 1112\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/openai/_base_client.py:859\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    851\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    852\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    858\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    860\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    861\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    862\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    863\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    864\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    865\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/openai/_base_client.py:911\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    908\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mEncountered Exception\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    910\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 911\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    912\u001b[0m         options,\n\u001b[1;32m    913\u001b[0m         cast_to,\n\u001b[1;32m    914\u001b[0m         retries,\n\u001b[1;32m    915\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    916\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    917\u001b[0m         response_headers\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    918\u001b[0m     )\n\u001b[1;32m    920\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRaising connection error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    921\u001b[0m \u001b[39mraise\u001b[39;00m APIConnectionError(request\u001b[39m=\u001b[39mrequest) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/openai/_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    980\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 982\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    983\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    984\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    985\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    986\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/openai/_base_client.py:934\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m    933\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 934\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    935\u001b[0m         options,\n\u001b[1;32m    936\u001b[0m         cast_to,\n\u001b[1;32m    937\u001b[0m         retries,\n\u001b[1;32m    938\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    939\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    940\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    941\u001b[0m     )\n\u001b[1;32m    943\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/openai/_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    980\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 982\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    983\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    984\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    985\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    986\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/paper_helper/lib/python3.11/site-packages/openai/_base_client.py:949\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    946\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m    948\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 949\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m    952\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m    953\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    957\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-B4eFQ7o7oEW1hnR69YX2eHCc on tokens per min (TPM): Limit 80000, Used 77628, Requested 4752. Please try again in 1.785s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "chain.invoke(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def divide_list(lst, n):\n",
    "    \"\"\"将列表 lst 均分成 n 个子列表\"\"\"\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be a positive integer\")\n",
    "\n",
    "    # 计算每个子列表的长度，使用整除和取模来处理不均等的情况\n",
    "    q, r = divmod(len(lst), n)\n",
    "    return [lst[i*q + min(i, r):(i+1)*q + min(i+1, r)] for i in range(n)]\n",
    "\n",
    "def query_openai_api(input, chain, delay=120):\n",
    "    results = chain.invoke(input)\n",
    "    time.sleep(delay)\n",
    "    return results\n",
    "\n",
    "# 输入数据列表\n",
    "inputs = divide_list(results, 5)\n",
    "print(len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 ThreadPoolExecutor 并行执行\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # 安排每个任务并设定不同的延迟\n",
    "    future_to_input = {executor.submit(query_openai_api, input_data, chain, delay=i*5): input_data for i, input_data in enumerate(inputs)}\n",
    "\n",
    "    # 收集所有任务的结果\n",
    "    results = []\n",
    "    for future in concurrent.futures.as_completed(future_to_input):\n",
    "        input_data = future_to_input[future]\n",
    "        try:\n",
    "            data = future.result()\n",
    "            results.append(data)\n",
    "        except Exception as exc:\n",
    "            print(f\"generated an exception: {exc}\")\n",
    "\n",
    "print(\"Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper_helper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
